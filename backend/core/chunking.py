# backend/core/chunking.py

from dataclasses import dataclass
from typing import List, Dict, Any, Tuple
from pathlib import Path
import uuid, re, tiktoken, os, logging

logger = logging.getLogger(__name__)

# Check Docling Availability
try:
    from docling.document_converter import DocumentConverter
    from docling.datamodel.base_models import InputFormat
    DOCLING_AVAILABLE = True
    logger.info(" Docling kh·∫£ d·ª•ng cho x·ª≠ l√Ω PDF/DOCX")
except ImportError:
    DOCLING_AVAILABLE = False
    logger.error(" Docling kh√¥ng kh·∫£ d·ª•ng! X·ª≠ l√Ω PDF/DOCX s·∫Ω th·∫•t b·∫°i.")

# Config
@dataclass
class ChunkConfig:
    max_tokens: int = 300
    overlap_tokens: int = 50
    lookback_ratio: float = 0.2
    table_rows_per_chunk: int = 50
    join_separator: str = "\n"

DocChunkConfig = ChunkConfig

# Constants
_SENTENCE_BREAK = re.compile(r"(?s)(.*?)([\\.!?‚Ä¶]|(?:\n{2,})|(?:\r?\n- )|(?:\r?\n‚Ä¢ ))\s+$")

class DoclingExtractor:
    """Tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ PDF/DOCX b·∫±ng Docling"""
    
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
        
        if not DOCLING_AVAILABLE:
            raise ImportError("‚ùå Docling ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t!")
        
        try:
            self.converter = DocumentConverter()
            logger.info(" ƒê√£ kh·ªüi t·∫°o Docling (s·ª≠ d·ª•ng API m·ªõi nh·∫•t)")
        except TypeError:
            try:
                from docling.datamodel.pipeline_options import PdfPipelineOptions
                device = os.getenv('DOCLING_DEVICE', 'cpu')
                do_ocr = os.getenv('DOCLING_OCR', 'false').lower() == 'true'
                
                pipeline_options = PdfPipelineOptions()
                pipeline_options.do_ocr = do_ocr
                pipeline_options.do_table_structure = True
                
                self.converter = DocumentConverter(
                    allowed_formats=[InputFormat.PDF, InputFormat.DOCX],
                    pipeline_options=pipeline_options
                )
                logger.info(f"‚úÖ ƒê√£ kh·ªüi t·∫°o Docling ( thi·∫øt b·ªã={device}, ocr={do_ocr})")
            except Exception as e:
                logger.error(f"‚ùå Kh·ªüi t·∫°o Docling th·∫•t b·∫°i: {e}")
                raise RuntimeError(f"Failed to initialize Docling: {e}")
        
        self._initialized = True
    
    def extract(self, filepath: str) -> Tuple[str, dict]:
        """Tr√≠ch xu·∫•t vƒÉn b·∫£n v√† metadata t·ª´ file"""
        try:
            logger.info(f"üìÑ Docling ƒëang tr√≠ch xu·∫•t: {Path(filepath).name}")
            result = self.converter.convert(filepath)
            metadata = self._extract_metadata(result)
            
            # Th·ª≠ c√°c ph∆∞∆°ng th·ª©c xu·∫•t: markdown -> text -> custom
            for method, func in [
                ("markdown", lambda: result.document.export_to_markdown()),
                ("text", lambda: result.document.export_to_text()),
                ("custom", lambda: self._custom_export(result))
            ]:
                try:
                    text = func()
                    if text and text.strip():
                        metadata['export_method'] = method
                        logger.info(f"‚úÖ Xu·∫•t {method}: {len(text)} k√Ω t·ª±, {len(text.split())} t·ª´")
                        return text, metadata
                except Exception as e:
                    if method == "custom":
                        raise
                    logger.warning(f"‚ö†Ô∏è Xu·∫•t {method} th·∫•t b·∫°i: {e}")
            
            return "", metadata
        except Exception as e:
            logger.error(f"‚ùå Tr√≠ch xu·∫•t Docling th·∫•t b·∫°i cho {Path(filepath).name}: {e}")
            raise RuntimeError(f"Docling extraction failed: {e}")
    
    def _extract_metadata(self, result) -> dict:
        """Tr√≠ch xu·∫•t metadata t·ª´ t√†i li·ªáu"""
        metadata = {}
        try:
            doc = result.document
            
            # √Ånh x·∫° t√™n thu·ªôc t√≠nh sang key metadata
            attr_map = {
                'title': ['title', 'name'],
                'author': ['author', 'authors'],
                'created_date': ['created_date', 'creation_date'],
                'modified_date': ['modified_date'],
                'keywords': ['keywords'],
                'subject': ['subject'],
                'page_count': ['page_count', 'num_pages']
            }
            
            for key, attrs in attr_map.items():
                for attr in attrs:
                    if hasattr(doc, attr):
                        value = getattr(doc, attr)
                        if value:
                            if isinstance(value, list):
                                metadata[key] = ', '.join(value)
                            else:
                                metadata[key] = str(value)
                            break
            
            # G·ªôp metadata b·ªï sung n·∫øu c√≥
            if hasattr(doc, 'metadata') and isinstance(doc.metadata, dict):
                for k, v in doc.metadata.items():
                    if k not in metadata and v:
                        metadata[k] = str(v)
            
            if metadata:
                logger.info(f"üìã ƒê√£ tr√≠ch xu·∫•t {len(metadata)} tr∆∞·ªùng metadata: {list(metadata.keys())}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ tr√≠ch xu·∫•t metadata: {e}")
        
        return metadata
    
    def _custom_export(self, result) -> str:
        """Xu·∫•t vƒÉn b·∫£n c√≥ c·∫•u tr√∫c t√πy ch·ªânh"""
        lines = []
        
        # Element type to markdown mapping
        element_map = {
            "title": lambda t: f"# {t}",
            "section_header": lambda t: f"## {t}",
            "subtitle": lambda t: f"### {t}",
            **{f"heading_{i}": lambda t, i=i: f"{'#' * i} {t}" for i in range(1, 7)},
            **{f"h{i}": lambda t, i=i: f"{'#' * i} {t}" for i in range(1, 7)},
            "list_item": lambda t: f"- {t}",
            "numbered_list": lambda t: f"1. {t}",
            "ordered_list": lambda t: f"1. {t}",
            "bullet_list": lambda t: f"- {t}",
            "unordered_list": lambda t: f"- {t}",
            "code_block": lambda t: f"```\n{t}\n```",
            "code": lambda t: f"```\n{t}\n```",
            "pre": lambda t: f"```\n{t}\n```",
            "formula": lambda t: f"$$\n{t}\n$$",
            "equation": lambda t: f"$$\n{t}\n$$",
            "math": lambda t: f"$$\n{t}\n$$",
            "quote": lambda t: "\n".join(f"> {line}" for line in t.split("\n")),
            "block_quote": lambda t: "\n".join(f"> {line}" for line in t.split("\n")),
            "blockquote": lambda t: "\n".join(f"> {line}" for line in t.split("\n")),
            "figure": lambda t: f"![Figure: {t}]",
            "picture": lambda t: f"![Figure: {t}]",
            "image": lambda t: f"![Figure: {t}]",
            "page_header": lambda t: f"*[Header: {t}]*",
            "header": lambda t: f"*[Header: {t}]*",
            "page_footer": lambda t: f"*[Footer: {t}]*",
            "footer": lambda t: f"*[Footer: {t}]*",
            "reference": lambda t: f"[^{t}]",
            "citation": lambda t: f"[^{t}]",
            "bibliography": lambda t: f"[^{t}]",
            "text_box": lambda t: f" **Note:** {t}",
            "callout": lambda t: f" **Note:** {t}",
            "note": lambda t: f"**Note:** {t}",
            "caption": lambda t: f"*{t}*",
            "footnote": lambda t: f"[^{t}]",
            "paragraph": lambda t: t
        }
        
        try:
            if hasattr(result.document, 'iterate_items'):
                for element in result.document.iterate_items():
                    label = getattr(element, 'label', 'unknown')
                    text = getattr(element, 'text', '').strip()
                    
                    if not text:
                        continue
                    
                    # Handle tables specially
                    if label == "table":
                        if hasattr(element, 'export_to_dataframe'):
                            try:
                                df = element.export_to_dataframe()
                                lines.append(df.to_markdown(index=False))
                                logger.debug(f"‚úÖ B·∫£ng ƒë√£ xu·∫•t d·∫°ng markdown ({len(df)} d√≤ng)")
                            except Exception as e:
                                logger.warning(f"‚ö†Ô∏è Xu·∫•t b·∫£ng th·∫•t b·∫°i: {e}, s·ª≠ d·ª•ng vƒÉn b·∫£n th√¥")
                                lines.append(f"```\n{text}\n```")
                        else:
                            lines.append(f"```\n{text}\n```")
                    # Use mapping for other elements
                    elif label in element_map:
                        lines.append(element_map[label](text))
                    else:
                        logger.warning(f"‚ö†Ô∏è Lo·∫°i ph·∫ßn t·ª≠ kh√¥ng x√°c ƒë·ªãnh '{label}', gi·ªØ nguy√™n d·∫°ng vƒÉn b·∫£n")
                        lines.append(text)
                    
                    lines.append("")  # Add spacing
            
            if lines:
                return "\n".join(lines)
            
            # Fallback methods
            for attr in ['text', 'to_dict']:
                if hasattr(result.document, attr):
                    logger.warning(f"‚ö†Ô∏è S·ª≠ d·ª•ng document.{attr} d·ª± ph√≤ng")
                    if attr == 'text':
                        return result.document.text
                    else:
                        return str(result.document.to_dict().get('text', ''))
            
            logger.error("‚ùå T·∫•t c·∫£ ph∆∞∆°ng th·ª©c xu·∫•t ƒë·ªÅu th·∫•t b·∫°i")
            return ""
        except Exception as e:
            logger.error(f"‚ùå Xu·∫•t t√πy ch·ªânh th·∫•t b·∫°i: {e}")
            raise RuntimeError(f"Cannot extract text from document: {e}")


def _with_breadcrumb(section: str, content: str, part_idx: int, part_total: int) -> str:
    """Th√™m breadcrumb section v√†o chunk"""
    suffix = f" ‚Äî part {part_idx}/{part_total}" if part_total > 1 else ""
    return f"**[SECTION] {section}{suffix}**\n\n{content}"


def _soft_split(text: str, enc, max_size: int, overlap: int, lookback_ratio: float = 0.2) -> List[str]:
    """Chia vƒÉn b·∫£n th√¥ng minh theo ranh gi·ªõi c√¢u"""
    ids = enc.encode(text)
    n = len(ids)
    if n <= max_size:
        return [text]

    out, start = [], 0
    while start < n:
        end = min(start + max_size, n)
        window_ids = ids[start:end]
        window_text = enc.decode(window_ids)

        if end < n:
            lb_chars = max(10, int(len(window_text) * (1 - lookback_ratio)))
            tail = window_text[lb_chars:]
            m = _SENTENCE_BREAK.search(tail)
            if m:
                cut_char = lb_chars + m.end()
                window_ids = enc.encode(window_text[:cut_char])

        out.append(enc.decode(window_ids).rstrip())
        if start + len(window_ids) >= n:
            break
        start += len(window_ids) - overlap
    return out


def _split_table_text(header: str, rows: List[str], prefix: str, enc, max_tokens: int, overlap: int) -> List[str]:
    """Chia b·∫£ng v·ªõi header c·ªë ƒë·ªãnh"""
    sticky = header + "\n"
    chunks, cur_rows = [], []

    def emit():
        if not cur_rows:
            return
        body = "".join([r + "\n" for r in cur_rows])
        content = f"{prefix}\n\n{sticky}{body}".strip() if prefix else f"{sticky}{body}".strip()
        if len(enc.encode(content)) > max_tokens:
            chunks.extend(_soft_split(content, enc, max_tokens, overlap))
        else:
            chunks.append(content)

    for r in rows:
        candidate = cur_rows + [r]
        body = "".join([x + "\n" for x in candidate])
        content = f"{prefix}\n\n{sticky}{body}".strip() if prefix else f"{sticky}{body}".strip()
        if len(enc.encode(content)) <= max_tokens:
            cur_rows.append(r)
        else:
            emit()
            cur_rows = [r]
    emit()
    if not chunks:
        chunks.append(f"{prefix}\n\n{sticky}".strip() if prefix else sticky.strip())
    return chunks


def _split_table_markdown(text: str, enc, max_tokens: int, overlap: int) -> List[str]:
    """Chia b·∫£ng markdown th√¥ng minh"""
    lines = text.splitlines()
    block_start, block_end = None, None
    
    for i in range(len(lines) - 1):
        s1, s2 = lines[i].strip(), lines[i + 1].strip()
        if s1.startswith("|") and s1.endswith("|") and all(c in "-: " for c in s2.replace("|", "")):
            block_start, block_end = i, i + 2
            while block_end < len(lines) and lines[block_end].lstrip().startswith("|"):
                block_end += 1
            break
    
    if block_start is None:
        return _soft_split(text, enc, max_tokens, overlap)

    prefix = "\n".join(lines[:block_start]).strip()
    table_lines = lines[block_start:block_end]
    header, separator = table_lines[:2]
    rows = table_lines[2:]
    suffix = "\n".join(lines[block_end:]).strip()
    
    # Process table
    table_chunks = _split_table_text(header + "\n" + separator, rows, prefix, enc, max_tokens, overlap)
    
    # Append suffix
    if suffix:
        if table_chunks:
            last_chunk = table_chunks[-1]
            combined = last_chunk + "\n\n" + suffix
            if len(enc.encode(combined)) <= max_tokens:
                table_chunks[-1] = combined
            else:
                table_chunks.extend(_soft_split(suffix, enc, max_tokens, overlap))
        else:
            table_chunks = _soft_split(suffix, enc, max_tokens, overlap)
    
    return table_chunks


class Chunker:
    """Chia vƒÉn b·∫£n theo token v·ªõi theo d√µi c·∫•u tr√∫c heading"""
    
    def __init__(self, config: ChunkConfig = None):
        if config is None:
            try:
                from backend.config import Config
                config = ChunkConfig(
                    max_tokens=Config.DEFAULT_CHUNK_SIZE,
                    overlap_tokens=Config.DEFAULT_CHUNK_OVERLAP
                )
            except:
                config = ChunkConfig()
        
        self.config = config
        self.enc = tiktoken.encoding_for_model("gpt-4o-mini")
        self.heading_stack = []

    def count_tokens(self, text: str) -> int:
        """ƒê·∫øm s·ªë token trong vƒÉn b·∫£n"""
        return len(self.enc.encode(text))
    
    def _parse_heading(self, line: str) -> Tuple[int, str]:
        """Ph√¢n t√≠ch heading markdown, tr·∫£ v·ªÅ (level, title) ho·∫∑c (0, None)"""
        line = line.strip()
        if line.startswith('#'):
            level = len(line) - len(line.lstrip('#'))
            if level <= 6 and level < len(line) and line[level] == ' ':
                return (level, line[level:].strip())
        return (0, None)
    
    def _update_heading_stack(self, level: int, title: str):
        """C·∫≠p nh·∫≠t stack c·∫•u tr√∫c heading"""
        self.heading_stack = [h for h in self.heading_stack if h[0] < level]
        self.heading_stack.append((level, title))
    
    def _get_current_section(self) -> str:
        """L·∫•y ƒë∆∞·ªùng d·∫´n section hi·ªán t·∫°i t·ª´ heading stack"""
        return " > ".join(h[1] for h in self.heading_stack) if self.heading_stack else "ROOT"

    def chunk_text(self, text: str, filepath: str, section: str = "ROOT") -> List[Dict]:
        """Chia vƒÉn b·∫£n theo token, x·ª≠ l√Ω b·∫£ng v√† theo d√µi c·∫•u tr√∫c heading"""
        input_chars = len(text)
        input_tokens = self.count_tokens(text)
        logger.info(f"üìä Input: {input_chars} k√Ω t·ª±, {len(text.split())} t·ª´, {input_tokens} tokens")
        
        lines = text.split('\n')
        chunks, buf, buf_tokens, order = [], [], 0, 0
        self.heading_stack = []
        tiny_chunks_count = merged_chunks_count = 0

        def process_buffer():
            nonlocal order, tiny_chunks_count, merged_chunks_count
            if not buf:
                return []
            
            combined = "\n".join(buf).strip()
            pieces = _split_table_markdown(combined, self.enc, self.config.max_tokens, self.config.overlap_tokens)
            current_section = self._get_current_section()
            result = []
            
            for i, p in enumerate(pieces, 1):
                # Handle tiny chunks
                if len(p.strip()) < 5:
                    tiny_chunks_count += 1
                    logger.debug(f"üîπ Tiny chunk ({len(p)} chars): '{p[:30]}...'")
                    if chunks:
                        chunks[-1]['content'] += "\n" + p
                        chunks[-1]['tokens'] = self.count_tokens(chunks[-1]['content'])
                        merged_chunks_count += 1
                        logger.debug(f"‚úÖ Merged tiny chunk v√†o chunk tr∆∞·ªõc ({chunks[-1]['tokens']} tokens)")
                        continue
                    else:
                        logger.debug(f"‚ö†Ô∏è Gi·ªØ tiny chunk cho chunk ti·∫øp theo")
                        return [p]
                
                result.append({
                    "chunk_id": str(uuid.uuid4()),
                    "content": _with_breadcrumb(current_section, p, i, len(pieces)),
                    "tokens": self.count_tokens(p),
                    "order": order,
                    "file_path": filepath,
                    "file_type": Path(filepath).suffix[1:].upper(),
                    "section": current_section
                })
                order += 1
            return result

        for line in lines:
            level, title = self._parse_heading(line)
            if level > 0:
                self._update_heading_stack(level, title)
            
            line_tokens = self.count_tokens(line)
            if buf_tokens + line_tokens > self.config.max_tokens and buf:
                chunks.extend(process_buffer())
                # Keep overlap
                overlap_lines = buf[-5:] if len(buf) > 5 else buf
                buf = overlap_lines + [line]
                buf_tokens = sum(self.count_tokens(l) for l in buf)
            else:
                buf.append(line)
                buf_tokens += line_tokens

        # Process remaining buffer
        chunks.extend(process_buffer())
        
        output_chars = sum(len(c['content']) for c in chunks)
        output_tokens = sum(c['tokens'] for c in chunks)
        
        logger.info(f"üìä Output: {len(chunks)} chunks, {output_chars} k√Ω t·ª±, {output_tokens} tokens")
        logger.info(f"üîÑ Retention: {output_chars}/{input_chars} chars ({output_chars/max(input_chars,1)*100:.1f}%), "
                   f"{output_tokens}/{input_tokens} tokens ({output_tokens/max(input_tokens,1)*100:.1f}%)")
        
        if tiny_chunks_count > 0:
            logger.info(f"üîπ Tiny chunks: {tiny_chunks_count} ph√°t hi·ªán, {merged_chunks_count} ƒë√£ merge")
        
        retention_rate = output_chars / max(input_chars, 1)
        if retention_rate < 0.95:
            logger.warning(f"‚ö†Ô∏è M·∫•t {(1-retention_rate)*100:.1f}% n·ªôi dung trong qu√° tr√¨nh chunking!")

        return chunks


# H√†m tr√≠ch xu·∫•t file
def _read_file(filepath: str, encoding: str = 'utf-8') -> str:
    """H√†m ƒë·ªçc file chung v·ªõi encoding d·ª± ph√≤ng"""
    try:
        with open(filepath, 'r', encoding=encoding) as f:
            return f.read()
    except UnicodeDecodeError:
        if encoding != 'latin-1':
            return _read_file(filepath, 'latin-1')
        raise
    except Exception as e:
        logger.error(f"‚ùå L·ªói ƒë·ªçc file: {e}")
        return ""


def _extract_markdown(filepath: str) -> str:
    return _read_file(filepath)


def _extract_html(filepath: str) -> str:
    try:
        from bs4 import BeautifulSoup
        html = _read_file(filepath)
        soup = BeautifulSoup(html, 'html.parser')
        for script in soup(["script", "style"]):
            script.decompose()
        return soup.get_text(separator='\n', strip=True)
    except Exception as e:
        logger.error(f"‚ùå L·ªói tr√≠ch xu·∫•t HTML: {e}")
        return ""


def _extract_json(filepath: str) -> str:
    try:
        import json
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return json.dumps(data, indent=2, ensure_ascii=False)
    except Exception as e:
        logger.error(f"‚ùå L·ªói tr√≠ch xu·∫•t JSON: {e}")
        return ""


def _extract_xml(filepath: str) -> str:
    try:
        from bs4 import BeautifulSoup
        xml = _read_file(filepath)
        soup = BeautifulSoup(xml, 'xml')
        return soup.get_text(separator='\n', strip=True)
    except Exception as e:
        logger.error(f"‚ùå L·ªói tr√≠ch xu·∫•t XML: {e}")
        return ""


def _extract_text(filepath: str) -> str:
    return _read_file(filepath)


def _extract_excel(filepath: str) -> str:
    try:
        import pandas as pd
        dfs = pd.read_excel(filepath, sheet_name=None)
        text_parts = []
        enc = tiktoken.encoding_for_model("gpt-4o-mini")
        
        for sheet_name, df in dfs.items():
            if df.empty:
                continue
            header = " | ".join(str(col) for col in df.columns)
            rows = [" | ".join(map(str, row)) for row in df.values]
            sheet_prefix = f"=== Sheet: {sheet_name} ==="
            text_parts.extend(_split_table_text(header, rows, sheet_prefix, enc, max_tokens=300, overlap=50))
        return "\n\n".join(text_parts)
    except Exception as e:
        logger.error(f"‚ùå L·ªói tr√≠ch xu·∫•t Excel: {e}")
        return ""


def _extract_csv(filepath: str) -> str:
    try:
        import pandas as pd
        df = pd.read_csv(filepath)
        if df.empty:
            return ""
        enc = tiktoken.encoding_for_model("gpt-4o-mini")
        header = " | ".join(str(col) for col in df.columns)
        rows = [" | ".join(map(str, row)) for row in df.values]
        return "\n\n".join(_split_table_text(header, rows, "", enc, max_tokens=300, overlap=50))
    except Exception as e:
        logger.error(f"‚ùå L·ªói tr√≠ch xu·∫•t CSV: {e}")
        return ""


def _extract_pdf_pypdf2(filepath: str) -> str:
    """Tr√≠ch xu·∫•t PDF d·ª± ph√≤ng b·∫±ng PyPDF2"""
    try:
        import PyPDF2
        text_parts = []
        with open(filepath, 'rb') as f:
            reader = PyPDF2.PdfReader(f)
            for page_num, page in enumerate(reader.pages, 1):
                text = page.extract_text()
                if text.strip():
                    text_parts.append(f"=== Page {page_num} ===\n{text}")
        return "\n\n".join(text_parts)
    except Exception as e:
        logger.error(f"‚ùå L·ªói tr√≠ch xu·∫•t PDF b·∫±ng PyPDF2: {e}")
        return ""


def _extract_pdf_pdfplumber(filepath: str) -> str:
    """Tr√≠ch xu·∫•t PDF d·ª± ph√≤ng b·∫±ng pdfplumber"""
    try:
        import pdfplumber
        text_parts = []
        with pdfplumber.open(filepath) as pdf:
            for page_num, page in enumerate(pdf.pages, 1):
                text = page.extract_text()
                if text and text.strip():
                    text_parts.append(f"=== Page {page_num} ===\n{text}")
        return "\n\n".join(text_parts)
    except Exception as e:
        logger.error(f"‚ùå L·ªói tr√≠ch xu·∫•t PDF b·∫±ng pdfplumber: {e}")
        return ""


def _extract_pptx(filepath: str) -> str:
    try:
        from pptx import Presentation
        prs = Presentation(filepath)
        text_parts = []
        for idx, slide in enumerate(prs.slides, 1):
            slide_text = [shape.text.strip() for shape in slide.shapes 
                         if hasattr(shape, "text") and shape.text.strip()]
            if slide_text:
                text_parts.append(f"=== Slide {idx} ===\n" + "\n".join(slide_text))
        return "\n\n".join(text_parts)
    except Exception as e:
        logger.error(f"‚ùå L·ªói tr√≠ch xu·∫•t PPTX: {e}")
        return ""


def extract_text_from_file(filepath: str) -> Tuple[str, dict]:
    """Tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ file v·ªõi metadata
    
    Chi·∫øn l∆∞·ª£c:
    - PDF & DOCX: Th·ª≠ Docling tr∆∞·ªõc, d·ª± ph√≤ng PyPDF2/pdfplumber n·∫øu th·∫•t b·∫°i
    - ƒê·ªãnh d·∫°ng kh√°c: D√πng c√°c h√†m tr√≠ch xu·∫•t c≈©
    
    Tr·∫£ v·ªÅ:
        Tuple c·ªßa (vƒÉn_b·∫£n, metadata_dict)
    """
    ext = Path(filepath).suffix.lower()
    metadata = {}
    
    # PDF/DOCX: Th·ª≠ Docling v·ªõi d·ª± ph√≤ng
    if ext in ['.pdf', '.docx', '.doc']:
        # Try Docling first
        if DOCLING_AVAILABLE:
            try:
                logger.info(f"üìÑ ƒêang th·ª≠ Docling cho {ext}: {Path(filepath).name}")
                extractor = DoclingExtractor()
                text, metadata = extractor.extract(filepath)
                
                if text and text.strip():
                    logger.info(f"‚úÖ Docling ƒë√£ tr√≠ch xu·∫•t {len(text)} k√Ω t·ª± t·ª´ {Path(filepath).name}")
                    metadata['extraction_method'] = 'docling'
                    return text, metadata
                else:
                    logger.warning(f"‚ö†Ô∏è Docling tr·∫£ v·ªÅ vƒÉn b·∫£n r·ªóng, th·ª≠ fallback...")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Docling th·∫•t b·∫°i: {e}, ƒëang th·ª≠ fallback...")
        
        # D·ª± ph√≤ng ch·ªâ cho PDF
        if ext == '.pdf':
            # Try PyPDF2
            logger.info(f"üìÑ ƒêang th·ª≠ PyPDF2 cho PDF: {Path(filepath).name}")
            text = _extract_pdf_pypdf2(filepath)
            if text and text.strip():
                logger.info(f"‚úÖ PyPDF2 ƒë√£ tr√≠ch xu·∫•t {len(text)} k√Ω t·ª±")
                metadata['extraction_method'] = 'pypdf2'
                return text, metadata
            
            # Try pdfplumber
            logger.info(f"üìÑ ƒêang th·ª≠ pdfplumber cho PDF: {Path(filepath).name}")
            text = _extract_pdf_pdfplumber(filepath)
            if text and text.strip():
                logger.info(f"‚úÖ pdfplumber ƒë√£ tr√≠ch xu·∫•t {len(text)} k√Ω t·ª±")
                metadata['extraction_method'] = 'pdfplumber'
                return text, metadata
            
            raise RuntimeError(f"‚ùå T·∫•t c·∫£ ph∆∞∆°ng ph√°p tr√≠ch xu·∫•t PDF ƒë·ªÅu th·∫•t b·∫°i cho {Path(filepath).name}")
        
        # V·ªõi DOCX/DOC, Docling l√† b·∫Øt bu·ªôc
        raise RuntimeError(f"‚ùå Docling l√† b·∫Øt bu·ªôc cho {ext} nh∆∞ng ƒë√£ th·∫•t b·∫°i!")
    
    # ƒê·ªãnh d·∫°ng kh√°c: D√πng c√°c h√†m tr√≠ch xu·∫•t c≈©
    extractors = {
        '.md': _extract_markdown, '.markdown': _extract_markdown,
        '.html': _extract_html, '.htm': _extract_html,
        '.json': _extract_json,
        '.xml': _extract_xml,
        '.xlsx': _extract_excel, '.xls': _extract_excel,
        '.csv': _extract_csv,
        '.pptx': _extract_pptx, '.ppt': _extract_pptx,
    }
    
    # C√°c file vƒÉn b·∫£n
    text_exts = ['.txt', '.py', '.js', '.java', '.cpp', '.c', '.h', '.hpp', 
                 '.css', '.scss', '.sql', '.sh', '.bash', '.yml', '.yaml',
                 '.toml', '.ini', '.cfg', '.conf', '.log', '.r', '.rb', 
                 '.php', '.go', '.rs', '.swift', '.kt', '.ts', '.jsx', '.tsx']
    
    for text_ext in text_exts:
        extractors[text_ext] = _extract_text
    
    if ext in extractors:
        return extractors[ext](filepath), metadata
    
    logger.warning(f"‚ö†Ô∏è Lo·∫°i file kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£: {ext}")
    return "", metadata


def process_document_to_chunks(filepath: str, config: ChunkConfig = None) -> Tuple[List[Dict], dict]:
    """ƒêi·ªÉm v√†o ch√≠nh cho x·ª≠ l√Ω t√†i li·ªáu
    
    Tham s·ªë:
        filepath: ƒê∆∞·ªùng d·∫´n t·ªõi t√†i li·ªáu
        config: C·∫•u h√¨nh chunking (d√πng m·∫∑c ƒë·ªãnh t·ª´ Config n·∫øu None)
        
    Tr·∫£ v·ªÅ:
        Tuple c·ªßa (danh_s√°ch_chunks, metadata_dict)
    """
    if config is None:
        try:
            from backend.config import Config
            config = ChunkConfig(
                max_tokens=Config.DEFAULT_CHUNK_SIZE,
                overlap_tokens=Config.DEFAULT_CHUNK_OVERLAP
            )
        except:
            config = ChunkConfig()
    
    logger.info(f"üìÑ ƒêang x·ª≠ l√Ω: {filepath}")
    
    # Extract text
    text, metadata = extract_text_from_file(filepath)
    
    if not text.strip():
        logger.warning(f"‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c vƒÉn b·∫£n t·ª´ {filepath}")
        return [], metadata
    
    logger.info(f" ƒê√£ tr√≠ch xu·∫•t {len(text)} k√Ω t·ª±")
    
    # Chunk text
    chunker = Chunker(config)
    chunks = chunker.chunk_text(text, filepath)
    
    logger.info(f" ƒê√£ t·∫°o {len(chunks)} chunks")
    return chunks, metadata


__all__ = [
    'ChunkConfig',
    'DocChunkConfig',
    'Chunker',
    'DoclingExtractor',
    'extract_text_from_file',
    'process_document_to_chunks',
    'DOCLING_AVAILABLE'
]